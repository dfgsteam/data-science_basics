{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science I\n",
    "### Klausur I im Sommersemester 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allgemeine Informationen\n",
    "\n",
    "* Sie haben eine Woche Zeit, um die Klausur zu bearbeiten.\n",
    "\n",
    "* Sie können alle Quellen verwenden, müssen sie jedoch korrekt benennen. Wenn Sie ChatGPT oder eine ähnliche Software verwenden, müssen Sie dies kenntlich machen und den verwendeten Prompt angeben.\n",
    "\n",
    "* Sie sollten die folgenden Pakete verwenden: `numpy, pandas, scipy, geopy, scikit-learn/sklearn, matplotlib, seborn, openPyxl` und Pythons Standardlibraries. Diese sind ausreichend, um die Klausur zu lösen. Falls Sie andere Pakete verwenden, rechtfertigen Sie deren Verwendung.\n",
    "\n",
    "* Der Code muss ausreichend kommentiert und verständlich sein. Schreiben Sie Funktionen beim Wiederverwenden von Code. Befolgen Sie im Allgemeinen die Richtlinien aus der Vorlesung. Punkte können aufgrund eines schlecht strukturierten oder unverständlichen Codes abgezogen werden.\n",
    "\n",
    "* **Begründen Sie Entscheidungen** zur Auswahl von Plots, Hypothesentest usw. und **interpretieren Sie** Ihre Ergebnisse.\n",
    "\n",
    "* Sie dürfen in keiner Form Hilfe oder Rat von Dritten in Anspruch nehmen.\n",
    "\n",
    "* Bitte laden Sie Ihre vollständige Lösung der Klausur als `.zip`-Datei mit dem Dateinamen `vorname_matrikelnummer.zip` bis 8. August 2024 um 12:00 Uhr auf StudIP in den Ordner `Submission - Exam 1` hoch.\n",
    "\n",
    "* Fügen Sie der `.zip` Datei auch die unterschriebene Eigenständigkeitserklärung hinzu.\n",
    "\n",
    "* Wenn Sie Fragen haben, kontaktieren Sie uns bitte rechtzeitig über Rocketchat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, openpyxl, warnings, os, dsplotter, logging\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if debug:\n",
    "    logging.basicConfig(level=logging.ERROR)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgaben und Punkte:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th colspan=\"3\">Aufgabe 1 - Data Preprocessing</th>\n",
    "      <th colspan=\"2\">Aufgabe 2 - Plotting</th>\n",
    "      <th colspan=\"2\">Aufgabe 3 - Statistics</th>\n",
    "      <th colspan=\"2\">Aufgabe 4 - Machine Learning </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Aufgabe 1.1</th>\n",
    "      <th>Aufgabe 1.2</th>\n",
    "      <th>Aufgabe 1.3</th>\n",
    "      <th>Aufgabe 2.1</th>\n",
    "      <th>Aufgabe 2.2</th>\n",
    "      <th>Aufgabe 3.1</th>\n",
    "      <th>Aufgabe 3.2</th>\n",
    "      <th>Aufgabe 4.1</th>\n",
    "      <th>Aufgabe 4.2</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>10 Punkte </td>\n",
    "      <td>12 Punkte </td>\n",
    "      <td>2 Punkte </td>\n",
    "      <td>11 Punkte</td>\n",
    "      <td>27 Punkte </td>\n",
    "      <td>13 Punkte </td>\n",
    "      <td>5 Punkte </td>\n",
    "      <td>10 Punkte </td>\n",
    "      <td>10 Punkte </td>\n",
    "    </tr>\n",
    "    <!-- Add more rows as needed -->\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "## Aufgabe 0: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Klausurordner enthält ein `Dockerfile`, in dem alle relevanten Pakete definiert sind. Das `Dockerfile` baut auf dem Jupyter Server Image auf. Verwenden Sie dieses Dockerfile, um zuerst ein Docker Image zu erstellen und dann einen Docker Container von diesem Image zu starten. Benutzen Sie anschließend die Jupyter Server Instanz, um an der Klausur zu arbeiten. Wir empfehlen dringend, die Docker-Umgebung zu verwenden, um Versionskonflikte zwischen den verschiedenen Paketen zu vermeiden. Code, der in dieser Umgebung nicht ausführbar ist, wird als **nicht funktional** bewertet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Aufgabe 1: Data Preprocessing (24 Punkte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbeschreibung\n",
    "\n",
    "Im Ordner `data` finden Sie die monatlichen Parkdaten der Stadt Göttingen für das Jahr 2023 (Feb.-Dez.). Die Parkschein-Verkäufe an den stationären Parkscheinautomaten befinden sich in den Dateien, deren Namen mit `Cale` beginnt, und die mit der Parkster-App gekauften Parkscheine befinden sich in den Dateien, deren Namen mit `Parkster` beginnen.<br>\n",
    "Die Datei `parkzone_latlong.csv` enthält weitere geografische Informationen zu den Parkzonen und die Datei `psa_latlong.csv` enthält geografische Informationen über die Parkscheinautomaten innerhalb der Parkzone.\n",
    "\n",
    "Die bereitgestellten Parkdaten sind echte Rohdaten und stammen direkt von der Stadt Göttingen. Wir haben lediglich die geografischen Informationen hinzugefügt.\n",
    "\n",
    "*Bitte beachten Sie:*\n",
    "- *Obwohl wir nur Daten von Februar bis Dezember haben, bezeichnen wir diese im Folgenden als jährlich.*\n",
    "- *Aufgrund der Größe der Daten sollten Sie Ihren Arbeitsspeicher effizient verwenden. Vermeiden Sie daher die Speicherung mehrerer Kopien desselben DataFrames.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufagbe 1.1 - Laden der Daten (10 Punkte)\n",
    "Laden Sie die Dateien für die Parkscheinautomaten (`Cale-*`) und für die App (`Parkster-*`) und fügen Sie diese **jeweils** zu einem Dataframe zusammen, der die jährlichen Verkäufe für Parkscheinautomaten und App beinhaltet. <br>\n",
    "Laden Sie auch die weiteren Informationen zu den Parkscheinautomaten (`psa_latlong.csv`) und Parkzonen (` parkzones_latlong.csv`).\n",
    "\n",
    "Sie werden die Werte `0` und `999` in der Spalte `Automaten -ID` für die Daten der Parkscheinautomaten finden. <br> Ändern Sie die `0`en in `1`en und löschen Sie alle Einträge mit `999`.\n",
    "Überprüfen Sie auch auf Zeilen-Duplikate und löschen Sie diese gegebenenfalls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Automaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Parkscheinautomaten\n",
    "print(\"automaten starts\")\n",
    "automaten_files = list(filter(lambda x: x.startswith('Cale-'), os.listdir('data')))\n",
    "automaten_df = pd.concat([pd.read_excel(f\"data/{file}\", skiprows=2) for file in automaten_files])\n",
    "print(\"automaten finish\\n----------------\")\n",
    "\n",
    "if debug: print(automaten_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remote PA and convert number to int -> easier replacement + easier connection in task 1b\n",
    "automaten_df['Automat - Automaten ID'] = automaten_df['Automat - Automaten ID'].str.replace(\"PA\", \"\").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for App\n",
    "print(\"app start\")\n",
    "app_files = list(filter(lambda x: x.startswith('Parkster-'), os.listdir('data')))\n",
    "app_df = pd.concat([pd.read_excel(f\"data/{file}\") for file in app_files])\n",
    "app_df['Parkgebühren inkl. MwSt. in EUR'] = app_df['Parkgebühren inkl. MwSt. in EUR'].str.replace(',', '.').astype(float) # to compare floats\n",
    "print(\"app finish\\n----------------\")\n",
    "\n",
    "if debug: print(app_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Parkscheinautomaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Parkscheinautomaten information\n",
    "print(\"Parkscheinautomaten start\")\n",
    "psa_info_df = pd.read_csv('data/psa_latlong.csv')\n",
    "print(\"Parkscheinautomaten finish\\n----------------\")\n",
    "\n",
    "if debug: print(psa_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Parkzonen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for Parkzonen information\n",
    "print(\"Parkzonen start\")\n",
    "parkzone_info_df = pd.read_csv('data/parkzones_latlong.csv')\n",
    "print(\"Parkzonen finish\\n----------------\")\n",
    "if debug: print(parkzone_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove and replace Automaten-IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 0s to 1s in Automaten -ID column\n",
    "automaten_df['Automat - Automaten ID'] = automaten_df['Automat - Automaten ID'].replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop entry with id 999\n",
    "automaten_df = automaten_df[automaten_df['Automat - Automaten ID'] != 999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and drop duplicate rows\n",
    "automaten_df = automaten_df.drop_duplicates()\n",
    "app_df = app_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(automaten_df.columns)\n",
    "    print(app_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 1.2 - Zusammenführen und Formatieren (12 Punkte)\n",
    "Erstellen Sie einen DataFrame für beide Verkaufsarten, indem Sie die beiden zuvor erstellen DataFrames zusammenführen. Nutzen Sie dazu die Parkzonen Informationen *(in `parkzones_latlong.csv`)* und die Parkscheinautomatennummer *(in` pa_latlong.csv`)*. Stellen Sie sicher, dass sich in Ihrem DataFrame die geografischen Informationen für Parkscheinautomaten und Parkzonen befinden.\n",
    "Verwenden Sie die Spalten `Kaufdatum Lokal` und `Start` für das Kaufdatum, codieren Sie die Spalte als `datetime`-Objekt und verwenden Sie sie als Indexspalte. Stellen Sie außerdem sicher, dass die anderen Spalten ein angemessenes Datenformat haben.\n",
    "\n",
    "*Hinweis: Es ist zu erwarten, dass `Nan`-Werte für einige Spalten in den Zeilen zu Appkäufen auftauchen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "\n",
    "data = { # define structure\n",
    "    'time': 'datetime64[ns]',\n",
    "    'machine_ID': pd.Int64Dtype(),\n",
    "    'fee': 'float64',\n",
    "    'category': 'object',\n",
    "    'street': 'object',\n",
    "    'latitude_machine': 'float64',\n",
    "    'longitude_machine': 'float64',\n",
    "    'zone': 'int64',\n",
    "    'latitude_zone': 'float64',\n",
    "    'longitude_zone': 'float64'}\n",
    "\n",
    "df = pd.DataFrame(columns=data.keys()).astype(data) # init dataframe\n",
    "df.set_index('time', inplace=True) # set index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge automaten_df into main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the overlapping row_name to string -> better to merge\n",
    "automaten_df['Automat - Automaten ID'] = automaten_df['Automat - Automaten ID'].astype(str)\n",
    "psa_info_df['PSA'] = psa_info_df['PSA'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp merge dataframe to create temp_df simpler and more structured\n",
    "temp_merged_df = pd.merge(automaten_df, psa_info_df, left_on='Automat - Automaten ID', right_on='PSA', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init temp df to merge better into main df\n",
    "temp_df = pd.DataFrame({\n",
    "    'time': pd.to_datetime(temp_merged_df['Kaufdatum Lokal'], format='%d.%m.%Y %H:%M:%S'),\n",
    "    'machine_ID': temp_merged_df['Automat - Automaten ID'].astype(int),\n",
    "    'fee': temp_merged_df['Betrag'],\n",
    "    'category': 'machine',\n",
    "    'street': temp_merged_df['location'],\n",
    "    'latitude_machine': temp_merged_df['latitude'],\n",
    "    'longitude_machine': temp_merged_df['longitude'],\n",
    "    'zone': temp_merged_df['zone'],\n",
    "    'latitude_zone': np.nan,\n",
    "    'longitude_zone': np.nan\n",
    "})\n",
    "\n",
    "temp_df.set_index('time', inplace=True) # set time as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "if debug:\n",
    "    print(temp_df)\n",
    "    print(temp_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge temp_df into main df\n",
    "df = pd.concat([df, temp_df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "if debug:\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merge app_df into main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp merge dataframe to create temp_df simpler and more structured\n",
    "merged_temp_df = pd.merge(app_df, parkzone_info_df, left_on='Parkzone', right_on='Zonencode', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init temp df to merge better into main df\n",
    "additional_df = pd.DataFrame({\n",
    "    'time': pd.to_datetime(merged_temp_df['Start'], format='%Y-%m-%d %H:%M:%S'),\n",
    "    'machine_ID': pd.NA,\n",
    "    'fee': merged_temp_df['Parkgebühren inkl. MwSt. in EUR'],\n",
    "    'category': 'app',\n",
    "    'street': np.nan,\n",
    "    'latitude_machine': np.nan,\n",
    "    'longitude_machine': np.nan,\n",
    "    'zone': merged_temp_df['Parkzone'],\n",
    "    'latitude_zone': merged_temp_df['latitude'],\n",
    "    'longitude_zone': merged_temp_df['longitude']\n",
    "})\n",
    "\n",
    "additional_df.set_index('time', inplace=True)  # set time as index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "if debug:\n",
    "    print(merged_temp_df)\n",
    "    print(merged_temp_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge temp_df into main df\n",
    "df = pd.concat([df, additional_df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "if debug:\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 1.3 - DataFrame Check (2 Punkte)\n",
    "Der bereinigte und vollständige DataFrame für die folgenden Aufgaben sollte der Datei `data/clean_dataframe.csv` entsprechen, der wie folgt eingelesen werden kann:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and Sort clean_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.read_csv('data/clean_dataframe.csv', parse_dates=['time'], index_col='time', dtype={\n",
    "                                                                            'machine_ID': 'Int64', \n",
    "                                                                            'fee': 'float64', \n",
    "                                                                            'category': 'object', \n",
    "                                                                            'street': 'object', \n",
    "                                                                            'latitude_machine': 'float64', \n",
    "                                                                            'longitude_machine': 'float64', \n",
    "                                                                            'zone': 'int64', \n",
    "                                                                            'latitude_zone': 'float64', \n",
    "                                                                            'longitude_zone': 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug: print(df_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_compare.sort_values(by=['time', 'fee', 'zone', 'machine_ID'])\n",
    "df = df.sort_values(by=['time', 'fee', 'zone', 'machine_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(df)\n",
    "    print(df_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### compare dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.equals(df_compare))\n",
    "\n",
    "# Unterschiede anzeigen\n",
    "differences = df.compare(df_compare)\n",
    "\n",
    "for col in differences.columns.levels[0]:\n",
    "    diff = differences[col]\n",
    "    if not diff.dropna().empty:\n",
    "        print(f\"Unterschiede in der Spalte '{col}':\")\n",
    "        print(diff.dropna())\n",
    "        print()\n",
    "        \n",
    "# Ergebnis anzeigen\n",
    "print(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stellen Sie sicher, dass Ihr DataFrame mit `clean_dataframe.csv` übereinstimmt. Verwenden Sie dazu die Funktion [`pandas.DataFrame.equals`](https://pandas.pydata.org/docs/reference/api/pandas.dataframe.equals.html).\n",
    "\n",
    "Sollte `pandas.DataFrame.equals` nach Ihren Anpassungen nicht `True` zurückgeben, arbeiten Sie bitte mit `clean_dataframe.csv` weiter und geben Sie dies in einer Markdown-Zelle an. In diesem Fall erhalten Sie keine Punkte für die Teilaufgabe 1.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "## Aufgabe 2: Plotting (38 Punkte)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean_dataframe.csv', parse_dates=['time'], index_col='time', dtype={\n",
    "                                                                            'machine_ID': 'Int64', \n",
    "                                                                            'fee': 'float64', \n",
    "                                                                            'category': 'object', \n",
    "                                                                            'street': 'object', \n",
    "                                                                            'latitude_machine': 'float64', \n",
    "                                                                            'longitude_machine': 'float64', \n",
    "                                                                            'zone': 'int64', \n",
    "                                                                            'latitude_zone': 'float64', \n",
    "                                                                            'longitude_zone': 'float64'})\n",
    "df = df[df.index.year == 2023]\n",
    "df_machines = df[df['category'] == 'machine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psa_latlong = pd.read_csv('data/psa_latlong.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.1 - Analyse der Parkscheinautomaten (11 Punkte)\n",
    "Die Stadt Göttingen möchte einen Überblick über die Umsätze der einzelnen **Parkscheinautomaten** erhalten und stellt Sie für eine anfängliche explorative Analyse des Verkaufsvolumens und der geografischen Anordnung der Automaten ein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 (6 Punkte)\n",
    "Finden Sie die fünf umsatzstärksten Parkscheinautomaten im Jahr 2023 und visualisieren Sie den **wöchentlichen** Umsatz im Laufe des Jahres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by week and calculate the total sales for each parking meter\n",
    "weekly_sales = df_machines.groupby([pd.Grouper(freq='W-MON'), 'machine_ID'])['fee'].sum().reset_index()\n",
    "\n",
    "# Sort the parking meters by total sales in descending order\n",
    "top_5_automaten = weekly_sales.groupby('machine_ID')['fee'].sum().nlargest(5).index\n",
    "\n",
    "# Filter the data for the top 5 parking meters\n",
    "top_5_sales = weekly_sales[weekly_sales['machine_ID'].isin(top_5_automaten)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Die Top-5 Automaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_5_sales.groupby(\"machine_ID\")[\"fee\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verlauf über das Jahr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weekly sales for the top 5 parking meters\n",
    "plt.figure(figsize=(12, 6))\n",
    "for machine_id, data in top_5_sales.groupby('machine_ID'):\n",
    "    plt.plot(data['time'], data['fee'], label=f'MachineID {machine_id}')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Weekly Sales for Top 5 Revenue-Generating Parking Meters in 2023')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zusammenfassung**\n",
    "\n",
    "- Zeigt den wöchentlichen Umsatz der fünf umsatzstärksten Parkscheinautomaten im Jahr 2023.\n",
    "- Jeder Automat ist durch eine eigene Farbe und Linie dargestellt.\n",
    "- Schwankungen im Umsatzverlauf des Jahres sind sichtbar.\n",
    "- Automat mit MachineID 53 hat durchgehend hohe Umsätze.\n",
    "\n",
    "**Begründung der Visualisierung**\n",
    "\n",
    "Eine Liniengrafik wurde gewählt, um den wöchentlichen Umsatz der Parkscheinautomaten darzustellen, da sie einen klaren Vergleich der Umsatztrends im Zeitverlauf ermöglicht. Diese Darstellung macht es einfach, Muster und Unterschiede zwischen den Automaten zu erkennen und die zeitliche Entwicklung sowie Schwankungen im Umsatz zu visualisieren. Dadurch können Spitzen und Täler leicht identifiziert werden, was Hinweise auf saisonale Effekte oder besondere Ereignisse geben kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 (5 Punkte)\n",
    "Der Standort der Parkscheinautomaten könnte auch einen Einfluss auf deren Umsatz haben.\n",
    "\n",
    "Machen Sie sich mit der Funktion `plot_map` aus der Bibliothek `dsplotter` vertraut. Verwenden Sie die Funktion, um den jährlichen Umsatz für jeden Automaten auf einer Karte zu visualisieren. Machen Sie die Farbe **und** den Radius der Standortmarkierung abhängig vom jährlichen Umsatz. Was haben Automaten mit einem hohen jährlichen Umsatz gemeinsam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the annual turnover for each vending machine\n",
    "annual_revenue = df_machines.groupby(['machine_ID', 'latitude_machine', 'longitude_machine'])['fee'].sum().reset_index()\n",
    "\n",
    "# Preparation of data for visualization\n",
    "total_revenue = annual_revenue.groupby(['machine_ID', 'latitude_machine', 'longitude_machine'])['fee'].sum().reset_index()\n",
    "total_revenue.columns = ['machine_ID', 'latitude_machine', 'longitude_machine', 'total_fee']\n",
    "\n",
    "# Creating the map with plot_map\n",
    "dsplotter.plot_map(\n",
    "    data=total_revenue,\n",
    "    color_col='total_fee',\n",
    "    radius_col='total_fee',\n",
    "    radius_scale=15,\n",
    "    alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zusammenfassung**\n",
    "\n",
    "- Alle Automaten, welche höhere Jahresumsätze verzeichnen, befinden sich alle in der Innenstadt/Kern von Göttingen und bei der Universität.\n",
    "    - Abseits des Universitätsautomaten, sind die Kosten pro Stunde in der Innenstadt ebenfalls höher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2.2 - Analyse der Automaten- und Appnutzung pro Parkzone (27 Punkte)\n",
    "Im Rahmen der Digitalisierungsinitiative der Stadt wurde die Parkster-App vor einigen Jahren als Alternative zu Parkscheinautomaten eingeführt.\n",
    "Bisher wurden nur die Parkscheinautomaten in die Analyse eingeschlossen und daher einen Großteil des Ticketverkaufs, der über die App stattfand, nicht beachtet.\n",
    "\n",
    "Die Stadt möchte für 2023 eine erste visuelle Analyse der Akzeptanz der App in den einzelnen Parkzonen durchführen und anschließend den gesamten Umsatz analysieren.\n",
    "\n",
    "*Hinweis: Beachten Sie, dass wir die Umsätze aus der Appnutzung nur einschließen können, indem wir die Gesamtauswertung auf Ebene der Parkzonen durchführen.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 (6 Punkte)\n",
    "Bevorzugen Parkende die App- oder die Parkscheinautomatennutzung? \n",
    "\n",
    "Verwenden Sie einen geeigneten Plot, um die durchschnittliche Automaten- bzw. Appnutzungsrate pro **Parkzone** für das gesamte Jahr 2023 zu visualisieren. Was können Sie dem Plot entnehmen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the usage rates per park zone\n",
    "usage_rate = df.groupby(['zone', 'category'])['fee'].count().unstack().fillna(0)\n",
    "usage_rate['total'] = usage_rate.sum(axis=1)\n",
    "usage_rate['app_rate'] = usage_rate['app'] / usage_rate['total']\n",
    "usage_rate['machine_rate'] = usage_rate['machine'] / usage_rate['total']\n",
    "\n",
    "# Sorting zones by zone number\n",
    "usage_rate = usage_rate.sort_index()\n",
    "\n",
    "# Plotting the absolute usage per park zone for the entire year 2023\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 16))\n",
    "width = 0.35  # the width of the bars\n",
    "indices = np.arange(len(usage_rate))\n",
    "\n",
    "# Plotting the absolute usage\n",
    "app_bars = ax1.bar(indices - width/2, usage_rate['app'], width, label='App', color='skyblue')\n",
    "machine_bars = ax1.bar(indices + width/2, usage_rate['machine'], width, label='Machine', color='salmon')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax1.set_xlabel('Park Zone', fontsize=14)\n",
    "ax1.set_ylabel('Number of Uses', fontsize=14)\n",
    "ax1.set_title('Absolute App and Machine Usage per Park Zone in 2023', fontsize=16)\n",
    "ax1.set_xticks(indices)\n",
    "ax1.set_xticklabels(usage_rate.index, rotation=45, ha='right', fontsize=12)\n",
    "ax1.legend(fontsize=12)\n",
    "\n",
    "# Adding value labels to the absolute plot\n",
    "def add_labels(bars, ax):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.0f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "add_labels(app_bars, ax1)\n",
    "add_labels(machine_bars, ax1)\n",
    "\n",
    "# Plotting the relative usage per park zone for the entire year 2023\n",
    "app_rate_bars = ax2.bar(indices - width/2, usage_rate['app_rate'], width, label='App', color='skyblue')\n",
    "machine_rate_bars = ax2.bar(indices + width/2, usage_rate['machine_rate'], width, label='Machine', color='salmon')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax2.set_xlabel('Park Zone', fontsize=14)\n",
    "ax2.set_ylabel('Usage Rate', fontsize=14)\n",
    "ax2.set_title('Relative App and Machine Usage per Park Zone in 2023', fontsize=16)\n",
    "ax2.set_xticks(indices)\n",
    "ax2.set_xticklabels(usage_rate.index, rotation=45, ha='right', fontsize=12)\n",
    "ax2.legend(fontsize=12)\n",
    "\n",
    "# Adding value labels to the relative plot\n",
    "def add_labels_relative(bars, ax):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "add_labels_relative(app_rate_bars, ax2)\n",
    "add_labels_relative(machine_rate_bars, ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Die durchschnittliche Appnutzungsrate beträgt {int(usage_rate['app_rate'].mean()*100)}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begründung**\n",
    "\n",
    "1. Bevorzugung der Automaten (Absolute Nutzung, Oberes Diagramm):\n",
    "- Hochfrequentierte Zonen: In den Zonen 37001, 37005, 37009 werden Automaten viel häufiger genutzt als die App.\n",
    "- Dominanz der Automaten: Insgesamt zeigen die meisten Zonen eine deutlich höhere Nutzung der Automaten im Vergleich zur App.\n",
    "\n",
    "2. Relative Nutzung (Unteres Diagramm):\n",
    "- Automaten bevorzugt: In den meisten Zonen sind die Automaten dominanter.\n",
    "- App-Nutzung in einigen Zonen: Einige Zonen wie 37102 und 37106 zeigen eine hohe relative Nutzung der App, teils sogar dominierend.\n",
    "\n",
    "**Zusammenfassung**\n",
    "- Automaten werden generell bevorzugt, vor allem in hochfrequentierten Zonen. (63% zu 37% -> Automat zu App)\n",
    "- Einige Zonen zeigen eine Präferenz für die App, was auf spezifische Nutzerpräferenzen oder bessere App-Promotion hinweisen könnte.\n",
    "\n",
    "Diese Informationen helfen, strategische Entscheidungen für die Förderung der App-Nutzung oder die Verbesserung der Automaten zu treffen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begründung für die Darstellung**\n",
    "\n",
    "Diese Darstellung wurde gewählt, um die Nutzungsmuster von Parkscheinautomaten und Apps in verschiedenen Parkzonen zu vergleichen. Das obere Diagramm zeigt die absolute Anzahl der Transaktionen, um die Gesamtbelastung jeder Zone zu verdeutlichen, während das untere Diagramm die relative Nutzungsrate darstellt, um die Präferenz der Nutzer zwischen App und Automat zu visualisieren. Diese Kombination ermöglicht eine umfassende Analyse der Nutzungsmuster und hilft dabei, gezielte Verbesserungen der Parkinfrastruktur vorzunehmen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 (9 Punkte)\n",
    "Wie stark werden die einzelnen Parkzonen genutzt? \n",
    "\n",
    "Visualisieren Sie die Gesamtzahl der Verkäufe und die Automaten- bzw. Appnutzungrate für jede Parkzone im Jahr 2023 **in einem Diagramm**. Verwenden Sie für die y-Achse eine `log`-Skalierung. Was können Sie dem Plot entnehmen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the usage rates per park zone\n",
    "usage_rate = df.groupby(['zone', 'category'])['fee'].count().unstack().fillna(0)\n",
    "usage_rate['total'] = usage_rate.sum(axis=1)\n",
    "usage_rate['app_rate'] = usage_rate['app'] / usage_rate['total']\n",
    "usage_rate['machine_rate'] = usage_rate['machine'] / usage_rate['total']\n",
    "\n",
    "# Sorting zones by zone number\n",
    "usage_rate = usage_rate.sort_index()\n",
    "\n",
    "# Plotting the total sales and usage rates per park zone for the entire year 2023\n",
    "fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "width = 0.4  # the width of the bars\n",
    "indices = np.arange(len(usage_rate))\n",
    "\n",
    "# Plotting the total sales with log scale\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Park Zone', fontsize=14)\n",
    "ax1.set_ylabel('Total Sales (log scale)', color=color, fontsize=14)\n",
    "bars = ax1.bar(indices, usage_rate['total'], width, color=color, alpha=0.6)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Adding value labels to the bars\n",
    "def add_labels(bars, ax):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.0f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "add_labels(bars, ax1)\n",
    "\n",
    "# Creating a second y-axis for the usage rates\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Usage Rate', color=color, fontsize=14)\n",
    "ax2.plot(indices, usage_rate['app_rate'], color='skyblue', marker='o', label='App Rate')\n",
    "ax2.plot(indices, usage_rate['machine_rate'], color='salmon', marker='x', label='Machine Rate')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(loc='upper right', fontsize=12)\n",
    "\n",
    "# Adding some titles and formatting\n",
    "plt.title('Total Sales and Usage Rates per Park Zone in 2023', fontsize=16)\n",
    "ax1.set_xticks(indices)\n",
    "ax1.set_xticklabels(usage_rate.index, rotation=45, ha='right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begrüdung**\n",
    "\n",
    "Der Plot, der die Gesamtzahl der Verkäufe und die App- bzw. Automaten-Nutzungsrate pro Parkzone im Jahr 2023 zeigt, bietet folgende Erkenntnisse:\n",
    "\n",
    "Gesamtzahl der Verkäufe (Balken, linke y-Achse, log-Skalierung):\n",
    "- Hohe Nutzung: Parkzonen 37001, 37005, 37008 haben die höchsten Verkaufszahlen, was auf eine intensive Nutzung hinweist.\n",
    "- Geringe Nutzung: Zonen wie 37105, 37106, 37107, 37207, 37208 zeigen deutlich niedrigere Verkaufszahlen.\n",
    "\n",
    "Automaten- und Appnutzungsrate (Linien, rechte y-Achse):\n",
    "- App-Nutzung: In einigen Zonen wie 37102, 37106, 37108 ist die App-Nutzungsrate höher (blaue Linie).\n",
    "- Automatennutzung: In den meisten Zonen, z.B. 37001, 37005, 37008, ist die Automatennutzung höher (rote Linie).\n",
    "\n",
    "**Zusammenfassung**\n",
    "1. Hohe Verkaufszahlen in zentralen Zonen: Zonen wie 37001 und 37005 sind stark genutzt, was auf zentrale oder beliebte Parkplätze hinweist.\n",
    "2. Bevorzugung der Automaten: In den meisten Zonen werden Automaten häufiger genutzt als die App, was auf Nutzerpräferenzen oder Verfügbarkeit hinweisen könnte.\n",
    "\n",
    "Diese Informationen sind wertvoll für die Planung und Optimierung von Parkzonen und der Zahlungsinfrastruktur. Es könnte sinnvoll sein, in Zonen mit hoher App-Nutzung die App weiter zu fördern, während in Zonen mit hoher Automaten-Nutzung die Automaten modernisiert oder zusätzlich auf App-Zahlungen hingewiesen wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begründung für die Darstellung**\n",
    "\n",
    "Diese Darstellung kombiniert zwei Informationen: die Gesamtzahl der Verkäufe pro Automat und die Automaten- bzw. Appnutzungsrate pro Parkzone im Jahr 2023. Der Plot nutzt eine logarithmische Skalierung für die y-Achse, um Unterschiede in den Verkaufszahlen besser darzustellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 (7 Punkte)\n",
    "Der vorherige Plot gibt uns eine Vorstellung von der Gesamtzahl der Parktickets *pro Parkzone*. Diese korreliert sehr wahrscheinlich stark mit der Anzahl der Parkplätze pro Zone. Um Parkzonen mit einer unterschiedlichen Anzahl an Parkplätzen zu vergleichen, sollten wir diese mithilfe der Anzahl der verfügbaren Parkplätze je Zone relativieren. Auf diese Weise können wir herausfinden, welche Zonen, relativ zu ihrer Größe, am häufigsten verwendet werden. Da wir nicht die Anzahl der Parkplätze für jede Zone zur Verfügung haben, können wir nur die Anzahl der Parkscheinautomaten als grobe Annäherung verwenden. \n",
    "\n",
    "Verwenden Sie die Informationen aus `psa_latlong.csv` und reproduzieren Sie den vorherigen mit Plot der Gesamtzahl der Verkäufe pro Automat für jede Parkzone. Verwenden Sie für die y-Achse eine `log`-Skalierung. Welche Parkzone wird am meisten genutzt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculation of the number of machines per parking zone\n",
    "automat_count_per_zone = psa_latlong['zone'].value_counts().sort_index()\n",
    "\n",
    "# Calculation of total sales per parking zone\n",
    "total_sales_per_zone = df.groupby('zone')['fee'].count()\n",
    "\n",
    "# Calculation of sales per machine per parking zone\n",
    "sales_per_automat = total_sales_per_zone / automat_count_per_zone\n",
    "\n",
    "# Creation of a DataFrame with the results\n",
    "sales_per_automat_df = sales_per_automat.reset_index()\n",
    "sales_per_automat_df.columns = ['zone', 'sales_per_automat']\n",
    "sales_per_automat_df = sales_per_automat_df.sort_values(by='zone')\n",
    "\n",
    "# Plotting the results\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "width = 0.4  # die Breite der Balken\n",
    "indices = np.arange(len(sales_per_automat_df))\n",
    "\n",
    "# Plot of sales per machine with logarithmic scaling of the y-axis\n",
    "color = 'tab:blue'\n",
    "ax.set_xlabel('Park Zone', fontsize=14)\n",
    "ax.set_ylabel('Total Sales per Automat (log scale)', color=color, fontsize=14)\n",
    "bars = ax.bar(indices, sales_per_automat_df['sales_per_automat'], width, color=color, alpha=0.6)\n",
    "ax.tick_params(axis='y', labelcolor=color)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Adding value labels to the bars\n",
    "def add_labels(bars, ax):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.0f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 Points vertical distance\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "add_labels(bars, ax)\n",
    "\n",
    "# Adding titles and formatting\n",
    "plt.title('Total Sales per Automat per Park Zone in 2023', fontsize=16)\n",
    "ax.set_xticks(indices)\n",
    "ax.set_xticklabels(sales_per_automat_df['zone'], rotation=45, ha='right', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernaussage:**\n",
    "- Verkäufe pro Automat: Dieser Plot zeigt, welche Parkzonen relativ zu ihrer Größe (basierend auf der Anzahl der Parkscheinautomaten) am häufigsten genutzt werden.\n",
    "- Am meisten genutzte Parkzone: Die Parkzone mit den höchsten Verkäufen pro Automat wird durch die Höhe des Balkens angezeigt.\n",
    "- Die Parkzone 37202 zeigt die höhste Nutzung auf. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begründung für die Darstellung**\n",
    "\n",
    "Ein Balkendiagramm wurde gewählt, da es Übersichtlichkeit und einfache Vergleichbarkeit der Verkaufszahlen pro Automat in verschiedenen Parkzonen bietet. Diese Darstellungsform ermöglicht es, Unterschiede in der Nutzung auf einen Blick zu erkennen und die absoluten Werte klar zu visualisieren. Zudem sind Balkendiagramme intuitiv und leicht verständlich, was die Interpretation der Daten für ein breites Publikum erleichtert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 (5 Punkte)\n",
    "Bisher haben wir die geografischen Informationen der Parkzonen nicht mit einbezogen. \n",
    "\n",
    "Verwenden Sie erneut die Funktion `plot_map`, um den Standort aller Parkzonen, ihre durchschnittlichen Tickets pro Automat und die Automaten- bzw. Appnutzungsrate zu visualieren. Färben Sie den Kartenmarker mithilfe der Automaten- bzw. Appnutzungsrate und legen Sie den Radius mit den durschnittlich verkauften Tickets pro Automat fest. Was können Sie der Darstellung entnehmen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculation of the number of machines per parking zone\n",
    "automat_count_per_zone = psa_latlong['zone'].value_counts().sort_index()\n",
    "\n",
    "# Calculation of total sales per parking zone\n",
    "total_sales_per_zone = df.groupby('zone')['fee'].count()\n",
    "\n",
    "# Calculation of sales per machine per parking zone\n",
    "sales_per_automat = total_sales_per_zone / automat_count_per_zone\n",
    "\n",
    "# Calculation of usage per category and parking zone\n",
    "usage_rate = df.groupby(['zone', 'category'])['fee'].count().unstack().fillna(0)\n",
    "usage_rate['total'] = usage_rate.sum(axis=1)\n",
    "usage_rate['app_rate'] = usage_rate['app'] / usage_rate['total']\n",
    "usage_rate['machine_rate'] = usage_rate['machine'] / usage_rate['total']\n",
    "\n",
    "# Combining the data\n",
    "combined_df = sales_per_automat.to_frame().join(usage_rate[['app_rate', 'machine_rate']])\n",
    "combined_df.reset_index(inplace=True)\n",
    "combined_df = combined_df.merge(psa_latlong.drop_duplicates('zone')[['zone', 'latitude', 'longitude']], on='zone')\n",
    "combined_df.columns = ['zone', 'sales_per_automat', 'app_rate', 'machine_rate', 'latitude', 'longitude']\n",
    "\n",
    "# Plotting the map with plot_map function\n",
    "dsplotter.plot_map(\n",
    "    data=combined_df,\n",
    "    color_col='app_rate',  # or 'machine_rate' to color by machine usage rate\n",
    "    radius_col='sales_per_automat',\n",
    "    radius_scale=10,\n",
    "    alpha=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernaussage:**\n",
    "1. Geografische Nutzungsmuster: Die Karte zeigt die geografische Verteilung der Parkzonen und wie stark sie genutzt werden.\n",
    "2. Nutzungsraten:** Durch die Farbgebung der Marker können Sie erkennen, in welchen Zonen die App-Nutzung bzw. die Automaten-Nutzung bevorzugt wird.\n",
    "3. Durchschnittliche Verkäufe pro Automat: Der Radius der Marker gibt Auskunft darüber, wie viele Tickets durchschnittlich pro Automat in einer Zone verkauft werden. Größere Marker deuten auf höhere Verkäufe hin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aufgabe 3: Statistics (18 Punkte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 3.1 - t-Test (13 Punkte)\n",
    "Zusätzlich zu der visuellen Analyse möchte die Stadt nun auch eine statistische Untersuchung der Verwendung von Parkscheinautomaten und Apps durchführen.\n",
    "\n",
    "Bestimmen Sie dazu zunächst die Automaten- bzw. Appnutzungsrate pro Parkzone für jeden Kalendertag. Führen Sie anschließend für **jede Parkzone** einen t-Test durch, der testet, ob Parkende es vorziehen die App in der jeweiligen Zone zu verwenden. Schreiben Sie das entsprechende Hypothesenpaar auf, führen Sie den Test durch und interpretieren Sie Ihre Testergebnisse. Verwenden Sie für Ihre Testentscheidung ein Signifikanzniveau von 0.05. Welche grundlegende Annahme von statistischen Tests könnte bei diesem Vorgehen verletzt werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Calculation of the usage rate per calendar day and parking zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the daily usage rate per parking zone\n",
    "daily_usage = df.groupby([df.index.date, 'zone', 'category'])['fee'].count().unstack().fillna(0)\n",
    "daily_usage['total'] = daily_usage.sum(axis=1)\n",
    "daily_usage['app_rate'] = daily_usage['app'] / daily_usage['total']\n",
    "daily_usage['machine_rate'] = daily_usage['machine'] / daily_usage['total']\n",
    "daily_usage.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Performing the t-test for each parking zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of zones\n",
    "zones = daily_usage['zone'].unique()\n",
    "\n",
    "# Hypothesis pair\n",
    "# H0: The average usage rate of the app is equal to that of the vending machines (no preference)\n",
    "# H1: The average usage rate of the app is not equal to that of the vending machines (there is a preference)\n",
    "\n",
    "t_test_results = []\n",
    "\n",
    "for zone in zones:\n",
    "    zone_data = daily_usage[daily_usage['zone'] == zone]\n",
    "    app_rate = zone_data['app_rate']\n",
    "    machine_rate = zone_data['machine_rate']\n",
    "    \n",
    "    t_stat, p_value = ttest_ind(app_rate, machine_rate)\n",
    "    \n",
    "    t_test_results.append({\n",
    "        'zone': zone,\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'reject_null': p_value < 0.05\n",
    "    })\n",
    "\n",
    "t_test_results_df = pd.DataFrame(t_test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Interpretation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the results of the t-test\n",
    "t_test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation\n",
    "Für jede Zone, für die die Nullhypothese abgelehnt wird (reject_null == True), kann gefolgert werden, dass es eine Präferenz für die App-Nutzung oder Automaten-Nutzung gibt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 3.2 - Statistisches Verständnis (5 Punkte)\n",
    "Angenommen, für die Zone `37106` beträgt die durchschnittliche Automaten- bzw. Appnutzungsrate `0.5`.\n",
    "Die Stadt sendet Ihnen die Daten für 2024. \n",
    "\n",
    "An wie vielen Tagen können Sie erwarten, dass die Appnutzung signifikant höher ist, wenn Sie weiterhin von einem Signifikanzniveau von `0.05` ausgehen? Erklären Sie, warum dies der Fall ist. Nehmen Sie an, dass sich das Verhalten der Parkenden im Vergleich zu 2023 nicht geändert hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of days in 2024\n",
    "num_days = 366\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Expected number of days\n",
    "expected_significant_days = num_days * alpha\n",
    "expected_significant_days\n",
    "\n",
    "# Calculate significance level\n",
    "expected_significant_days = 366 * 0.05\n",
    "print(expected_significant_days)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warum ist das der Fall?**\n",
    "\n",
    "1. Signifikanzniveau und Type-I-Fehler: Bei einem Signifikanzniveau von 0,05 akzeptieren wir eine 5%ige Wahrscheinlichkeit, die Nullhypothese fälschlicherweise abzulehnen. Dies bedeutet, dass bei jedem einzelnen Test eine 5%ige  Wahrscheinlichkeit besteht, dass wir fälschlicherweise eine Präferenz für die App-Nutzung feststellen.\n",
    "2. Viele Tests: Da wir jeden Tag einen Test durchführen (insgesamt 366 Tests), erwarten wir, dass 5% dieser Tests fälschlicherweise eine signifikante Präferenz zeigen. Diese 5% sind der erwartete Anteil der Tage, an denen der Type-I-Fehler auftritt.\n",
    "3. Unverändertes Verhalten: Wenn das Verhalten der Parkenden unverändert bleibt und die Nutzungsrate bei 0,5 bleibt, gibt es keine echte Präferenz. Daher basieren die signifikanten Ergebnisse ausschließlich auf zufälligen Schwankungen und dem festgelegten Signifikanzniveau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Aufgabe 4: Machine Learning (20 Punkte)\n",
    "\n",
    "Nutzen Sie ein K-Nearest-Neighbors (KNN) Modell, um die Automaten- bzw. Appnutzungsrate mithilfe des Standortes (`latitude`, `longitude`) und der Parkgebühr (`fee`) vorherzusagen. Verwenden Sie ausschließlich Datenreihen mit Parkgebühren zwischen 2 Euro und 7 Euro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 4.1 - Modell-Training and Hyperparameter-Suche (10 Punkte)\n",
    "Bereiten Sie die Daten sinnvoll auf, führen Sie eine Hyperparameter-Suche nach optimalem K-Wert aus, visualisieren Sie die Ergebnisse der Hyperparameter-Suche und verwenden Sie schließlich Ihren optimalen K-Wert, um das Modell zu trainieren.\n",
    "\n",
    "*Hinweis: Verwenden Sie 30% aller Daten zur Bestimmung des optimalen K-Wertes, um die Hyperparameter-Suche zu beschleunigen, und den gesamten Datensatz für das Modell-Training.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "data_training = df.drop([\"machine_ID\", \"street\", \"zone\", \"latitude_machine\", \"longitude_machine\"], axis=1)\n",
    "\n",
    "# Adapt data to the conditions and display the category in binary form\n",
    "data_training = data_training[(data_training[\"fee\"] <= 7) & (data_training[\"fee\"] >= 2)]\n",
    "data_training[\"binary_app\"] = (data_training[\"category\"] == \"app\").astype(int)\n",
    "\n",
    "# Use 30% of the data\n",
    "data_training_sampled = data_training.sample(frac=0.3, random_state=42)\n",
    "\n",
    "# Define samples, labels and number of folds\n",
    "samples = data_training_sampled[[\"latitude_zone\", \"longitude_zone\", \"fee\"]]\n",
    "labels = data_training_sampled[\"binary_app\"]\n",
    "folds = 5\n",
    "\n",
    "# Method to determine the folds for the samples and labels\n",
    "def create_folds(samples, labels, folds):\n",
    "    fold_size = len(samples) // folds\n",
    "    samples_folds = []\n",
    "    labels_folds = []\n",
    "    for i in range(folds):\n",
    "        fold_sample = samples.iloc[i * fold_size:(i + 1) * fold_size]\n",
    "        fold_label = labels.iloc[i * fold_size:(i + 1) * fold_size]\n",
    "        samples_folds.append(fold_sample)\n",
    "        labels_folds.append(fold_label)\n",
    "    return samples_folds, labels_folds\n",
    "\n",
    "samples_folds, labels_folds = create_folds(samples, labels, folds)\n",
    "\n",
    "acc_val = []\n",
    "\n",
    "# Iterate through the folds and use KNN to determine the best k-value\n",
    "for i in range(folds):\n",
    "    val_samples = samples_folds[i]\n",
    "    val_labels = labels_folds[i]\n",
    "    train_samples = pd.concat([s for j, s in enumerate(samples_folds) if j != i]).values\n",
    "    train_labels = pd.concat([l for j, l in enumerate(labels_folds) if j != i]).values\n",
    "\n",
    "    acc_v = []\n",
    "    for k in range(1, 10):\n",
    "        kneigh_cross = KNeighborsClassifier(n_neighbors=k)\n",
    "        kneigh_cross.fit(train_samples, train_labels)\n",
    "        acc_v.append(kneigh_cross.score(val_samples, val_labels))\n",
    "    acc_val.append(acc_v)\n",
    "\n",
    "acc_val = np.asarray(acc_val)\n",
    "acc_mean = [np.mean(acc_val[:, k-1]) for k in range(1, 10)]\n",
    "\n",
    "# Maximum accuracy and corresponding k output\n",
    "best_k = np.argmax(acc_mean) + 1\n",
    "print(f\"Größte Accuracy {np.max(acc_mean)} bei k = {best_k}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results to find the correct k\n",
    "plt.plot(range(1, 10), acc_mean, marker='o')\n",
    "plt.title('k vs. Cross-Validation Accuracy')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(range(1, 10))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model accuracy\n",
    "kneigh = KNeighborsClassifier(n_neighbors=best_k)\n",
    "kneigh.fit(samples, labels)\n",
    "print(f\"Model Accuracy mit k={best_k}: {kneigh.score(samples, labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 4.2 - Visualisierung der Modell-Vorhersage (10 Punkte)\n",
    "Erstellen Sie ein `100 x 100` Grid aus Längen-/Breitengradwerten unter Verwendung der minimalen und maximalen Werte in Ihrem Datensatz. Visualisieren Sie die Vorhersagen des KNN-Modells **für drei verschiedene Parkgebühren** - 3, 5 und 7 Euro. Verwenden Sie dazu die Funktion `plot_map`. Färben Sie den Marker entsprechend der Modell-Vorhersage. Beschreiben Sie mindestens 2 visuelle Veränderungen des vorhergesagten Nutzungsmusters auf der Karte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the limits of longitude and latitude\n",
    "longitude_bounds = (data_training[\"longitude_zone\"].min(), data_training[\"longitude_zone\"].max())\n",
    "latitude_bounds = (data_training[\"latitude_zone\"].min(), data_training[\"latitude_zone\"].max())\n",
    "\n",
    "# Generation of 100 evenly distributed points between the limit values\n",
    "longitudes = np.linspace(longitude_bounds[0], longitude_bounds[1], 100)\n",
    "latitudes = np.linspace(latitude_bounds[0], latitude_bounds[1], 100)\n",
    "\n",
    "# Creating a grid from the points\n",
    "grid_long, grid_lat = np.meshgrid(latitudes, longitudes)\n",
    "\n",
    "# Conversion of the grid into a DataFrame\n",
    "coordinates = np.c_[grid_long.ravel(), grid_lat.ravel()]\n",
    "coordinates_df = pd.DataFrame(coordinates, columns=[\"latitude_zone\", \"longitude_zone\"])\n",
    "\n",
    "# Forecast and visualization for different parking fees\n",
    "def predict_and_plot(grid_df, fee, model, title):\n",
    "    grid_df = grid_df.copy()\n",
    "    grid_df[\"fee\"] = fee\n",
    "    predictions = model.predict(grid_df)\n",
    "    grid_df[\"prediction\"] = predictions\n",
    "    print(title)\n",
    "    dsplotter.plot_map(grid_df, color_col=\"prediction\")\n",
    "\n",
    "# Forecast for parking fees -3, 5 and 7 and visualization\n",
    "fees_to_predict = [-3, 5, 7]\n",
    "titles = [\"Prediction for fee = -3\", \"Prediction for fee = 5\", \"Prediction for fee = 7\"]\n",
    "\n",
    "for fee, title in zip(fees_to_predict, titles):\n",
    "    predict_and_plot(coordinates_df, fee, kneigh, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Visualisierung repräsentiert Blau (0) die Nutzung von Parkscheinautomaten und Rot (1) die Nutzung der App.\n",
    "\n",
    "Bei einer Parkgebühr (fee) von -3 und 5 zeigt das Modell ausschließlich eine Vorhersage für die Nutzung von Parkscheinautomaten. Wenn die Gebühr auf 7 steigt, deutet das Modell darauf hin, dass im nördlichen Teil des Grids die App bevorzugt verwendet wird, während im südlichen Teil weiterhin die Automaten dominieren.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
